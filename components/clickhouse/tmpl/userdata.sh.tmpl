#!/usr/bin/env bash
set -euxo pipefail
exec > >(tee -a /var/log/user-data.log | logger -t user-data -s 2>/dev/console) 2>&1
echo "[userdata] start $(date -Is)"

# ------------- Timing helpers -----------------
SCRIPT_START_EPOCH=$(date +%s)
STEP_START_EPOCH=$SCRIPT_START_EPOCH

log_step() {
  local label="$1"
  local now_epoch
  now_epoch=$(date +%s)
  local now_iso
  now_iso=$(date -Is)
  local total=$$((now_epoch - SCRIPT_START_EPOCH))
  local delta=$$((now_epoch - STEP_START_EPOCH))
  echo "[userdata-clickhosue:obs][TIME] $${now_iso} $${label}: +$${delta}s (total $${total}s)"
  STEP_START_EPOCH=$$now_epoch
}

# make xtrace lines include timestamps + location
export PS4='+ $(date -Is) [$${BASH_SOURCE}:$${LINENO}] '

log_step "script entry"

# ---------------- Inputs from Terraform ----------------
export EBS_DEV="${EBS_DEVICE}"                     # "" => auto-pick largest non-root NVMe
export MNT="${MOUNT_POINT}"                        # e.g., "/var/lib/clickhouse"
export MARKER_FILE="${MARKER_FILE}"                # e.g., "/var/local/BOOTSTRAP_OK"
export CLICKHOUSE_HTTP="${CLICKHOUSE_HTTP_PORT}"   # e.g., 8123
export CLICKHOUSE_TCP="${CLICKHOUSE_TCP_PORT}"     # e.g., 9000
export CLICKHOUSE_BUCKET="${CLICKHOUSE_BUCKET}"    # e.g., "usekarma.dev-prod"
export CLICKHOUSE_PREFIX="${CLICKHOUSE_PREFIX}"    # e.g., "clickhouse"
export AWS_REGION="${AWS_REGION}"                  # e.g., "us-east-1"

# --- Remote scrape targets (filled by templatefile/TF) ---
export MONGO_HOST="${MONGO_HOST}"                  # e.g. "10.0.1.10"
export MONGO_EXP_PORT="${MONGO_EXP_PORT}"          # mongo exporter (percona) e.g. 9216
export MONGO_NODE_PORT="${MONGO_NODE_PORT}"        # node_exporter on mongo host (9100)

export MONGO_CONNECTION_STRING="${MONGO_CONNECTION_STRING}"

export REDPANDA_HOST="${REDPANDA_HOST}"            # e.g. "10.0.2.20"
export REDPANDA_EXP_PORT="${REDPANDA_EXP_PORT}"    # redpanda admin metrics (9644)
export REDPANDA_NODE_PORT="${REDPANDA_NODE_PORT}"  # node_exporter on redpanda host (9100)

export KCONNECT_HOST="${KCONNECT_HOST}"

# --- Versions passed from TF (do not default here) ---
export PROMETHEUS_VER="${PROMETHEUS_VER}"          # e.g., 2.55.1
export NODEEXP_VER="${NODEEXP_VER}"                # e.g., 1.8.2

export GRAFANA_DASHBOARD_KEY="${GRAFANA_DASHBOARD_KEY}"

log_step "env vars bound from Terraform"

# ---------------- Wait for basic network ----------------
for i in {1..30}; do curl -fsS https://aws.amazon.com >/dev/null && break || sleep 2; done
log_step "basic network ready (curl aws.amazon.com)"

# ---------------- Detect package manager / distro -------
PM=""
if command -v dnf >/dev/null 2>&1; then
  PM="dnf" ; DISTRO="al2023"
elif command -v yum >/dev/null 2>&1; then
  PM="yum" ; DISTRO="al2"
elif command -v apt-get >/dev/null 2>&1; then
  PM="apt" ; DISTRO="ubuntu"
else
  echo "ERROR: No supported package manager found (dnf/yum/apt)" >&2
  exit 1
fi

export PM
log_step "detected distro=$DISTRO pm=$PM"

# ---------------- Install & start SSM -------------------
if [[ "$PM" == "apt" ]]; then
  export DEBIAN_FRONTEND=noninteractive
  apt-get update -y
  apt-get install -y amazon-ssm-agent || true
else
  $PM -y update || true
  $PM -y install amazon-ssm-agent || true
fi
systemctl enable --now amazon-ssm-agent
systemctl is-active --quiet amazon-ssm-agent
log_step "amazon-ssm-agent installed and active"

# ---------------- Core tools ----------------------------
if [[ "$PM" == "apt" ]]; then
  apt-get install -y ca-certificates jq rsync xfsprogs xz-utils curl awscli zstd gnupg tar gzip netcat-openbsd
else
  $PM -y install ca-certificates jq rsync xfsprogs dnf-plugins-core awscli zstd gzip tar nmap-ncat --skip-broken || true
fi
log_step "core tools installed (jq/awscli/xfsprogs/etc)"

# ---------------- Optional: pick & mount data disk -------
if [[ -n "$MNT" ]]; then
  mkdir -p "$MNT"

  pick_data_dev() {
    if [[ -n "$EBS_DEV" && -b "$EBS_DEV" ]]; then echo "$EBS_DEV"; return 0; fi
    local rootpk
    rootpk="$(lsblk -no pkname "$(findmnt -no SOURCE /)" 2>/dev/null | head -n1)"
    lsblk -dn -o NAME,TYPE,SIZE \
    | awk '$2=="disk"{print $1" "$3}' \
    | sort -k2 -hr | awk '{print $1}' \
      | while read -r n; do
          [[ "$n" == "$rootpk" ]] && continue
          [[ -b "/dev/$n" ]] && { echo "/dev/$n"; break; }
        done
  }

  DATA_DEV=""
  for _ in {1..60}; do
    DATA_DEV="$(pick_data_dev || true)"
    [[ -n "$DATA_DEV" && -b "$DATA_DEV" ]] && break
    sleep 2
  done

  if [[ -n "$DATA_DEV" ]]; then
    if ! blkid "$DATA_DEV" >/dev/null 2>&1; then
      if [[ "$PM" == "apt" ]]; then apt-get install -y xfsprogs; else $PM -y install xfsprogs || true; fi
      mkfs.xfs -f "$DATA_DEV"
    fi
    UUID="$(blkid -s UUID -o value "$DATA_DEV")"
    sed -i "\|[[:space:]]$MNT[[:space:]]|d;/^\/dev\/xvdb[[:space:]]/d" /etc/fstab
    grep -q "$UUID" /etc/fstab || echo "UUID=$UUID  $MNT  xfs  defaults,nofail  0  2" >> /etc/fstab
    mount -a
    mountpoint -q "$MNT"
    log_step "data disk $DATA_DEV formatted/mounted at $MNT"
  else
    echo "WARN: No data disk found; ClickHouse will use default /var/lib/clickhouse" >&2
    log_step "no data disk found; using default /var/lib/clickhouse"
  fi
else
  log_step "MNT empty; skipping data disk mount"
fi

# ---------------- Install ClickHouse --------------------
if [[ "$PM" == "apt" ]]; then
  install -m 0755 -d /usr/share/keyrings
  curl -fsSL https://packages.clickhouse.com/apt/doc/apt-key.gpg \
    | gpg --dearmor -o /usr/share/keyrings/clickhouse.gpg
  echo "deb [signed-by=/usr/share/keyrings/clickhouse.gpg] https://packages.clickhouse.com/deb stable main" \
    > /etc/apt/sources.list.d/clickhouse.list
  apt-get update -y
  apt-get install -y clickhouse-server clickhouse-client
else
  dnf config-manager --add-repo https://packages.clickhouse.com/rpm/clickhouse.repo || true
  $PM -y makecache || true
  $PM -y install clickhouse-server clickhouse-client
fi
log_step "clickhouse-server and clickhouse-client installed"

# ---------------- Configure data path & ports -----------
systemctl stop clickhouse-server || true

TARGET_PATH="/var/lib/clickhouse"
if [[ -n "$MNT" && -n "$(mount | awk '{print $3}' | grep -Fx "$MNT" || true)" ]]; then
  TARGET_PATH="$MNT"
fi

mkdir -p "$TARGET_PATH" /var/lib/clickhouse
chown -R clickhouse:clickhouse "$TARGET_PATH"
rsync -a /var/lib/clickhouse/ "$TARGET_PATH"/ || true
chown -R clickhouse:clickhouse "$TARGET_PATH"

mkdir -p /etc/clickhouse-server/config.d

# (Terraform must not expand these)
cat >/etc/clickhouse-server/config.d/10-data-path.xml <<EOF
<clickhouse>
  <path>$${TARGET_PATH}/</path>
  <tmp_path>$${TARGET_PATH}/tmp/</tmp_path>
</clickhouse>
EOF

cat >/etc/clickhouse-server/config.d/20-network.xml <<EOF
<clickhouse>
  <listen_host>0.0.0.0</listen_host>
  <http_port>$${CLICKHOUSE_HTTP}</http_port>
  <tcp_port>$${CLICKHOUSE_TCP}</tcp_port>
</clickhouse>
EOF

log_step "clickhouse data path + ports configured (TARGET_PATH=$TARGET_PATH)"

# ---------------- Systemd region env (optional) ---------
mkdir -p /etc/systemd/system/clickhouse-server.service.d
cat >/etc/systemd/system/clickhouse-server.service.d/aws-env.conf <<EOF
[Service]
Environment=AWS_REGION=${AWS_REGION}
Environment=AWS_DEFAULT_REGION=${AWS_REGION}
EOF
log_step "systemd AWS env drop-in written"

# ---------------- Conditionally add S3 disk -------------
if [[ -n "${AWS_REGION}" && -n "${CLICKHOUSE_BUCKET}" && -n "${CLICKHOUSE_PREFIX}" ]]; then
  mkdir -p /var/lib/clickhouse/disks/s3_backups
  chown -R clickhouse:clickhouse /var/lib/clickhouse/disks
  cat >/etc/clickhouse-server/config.d/30-s3-backup.xml <<EOF
<clickhouse>
  <storage_configuration>
    <disks>
      <s3_backups>
        <type>s3_plain</type>
        <endpoint>https://$${CLICKHOUSE_BUCKET}.s3.$${AWS_REGION}.amazonaws.com/$${CLICKHOUSE_PREFIX}/backups/</endpoint>
        <use_environment_credentials>true</use_environment_credentials>
        <region>$${AWS_REGION}</region>
        <metadata_path>/var/lib/clickhouse/disks/s3_backups/</metadata_path>
      </s3_backups>
    </disks>
  </storage_configuration>
  <backups><allowed_disk>s3_backups</allowed_disk></backups>
</clickhouse>
EOF
  chmod 0644 /etc/clickhouse-server/config.d/30-s3-backup.xml
  log_step "s3_backups disk configured for clickhouse"
else
  echo "[userdata] Skipping S3 disk: require AWS_REGION, CLICKHOUSE_BUCKET, CLICKHOUSE_PREFIX"
  log_step "s3_backups disk config skipped (no AWS config)"
fi

# ---------------- Validate config & start ---------------
mkdir -p /var/lib/clickhouse /var/log/clickhouse-server
chown -R clickhouse:clickhouse /var/lib/clickhouse /var/log/clickhouse-server

if ! systemctl cat clickhouse-server | grep -q '^\[Install\]'; then
  cp -f /usr/lib/systemd/system/clickhouse-server.service /etc/systemd/system/clickhouse-server.service
  cat >>/etc/systemd/system/clickhouse-server.service <<'EOF'

[Install]
WantedBy=multi-user.target
EOF
else
  cp -f /usr/lib/systemd/system/clickhouse-server.service /etc/systemd/system/clickhouse-server.service || true
fi

systemctl daemon-reload

if ! systemctl enable --now clickhouse-server 2>/tmp/ch-enable.err; then
  if grep -q 'systemd-sysv-install' /tmp/ch-enable.err; then
    mkdir -p /etc/systemd/system/multi-user.target.wants
    ln -sf /etc/systemd/system/clickhouse-server.service \
           /etc/systemd/system/multi-user.target.wants/clickhouse-server.service
    systemctl start clickhouse-server
  else
    echo "[userdata] systemctl enable failed:"; cat /tmp/ch-enable.err
  fi
fi
rm -f /tmp/ch-enable.err || true

if ! systemctl is-active --quiet clickhouse-server; then
  echo "[userdata] ERROR: clickhouse-server not active"
  journalctl -u clickhouse-server -n 200 --no-pager || true
  log_step "clickhouse-server FAILED to become active"
  exit 1
fi

ss -lntp | egrep '(:8123|:9000|:9009)' || true
clickhouse-client -q "SELECT version(), now()"
log_step "clickhouse-server active and responding (ports + version check)"

# ---------------- ClickHouse restore-from-S3 (latest manual-*, once) ---
RESTORE_MARKER="/var/local/clickhouse_manual_restore_done"

if [[ -n "${AWS_REGION}" && -n "${CLICKHOUSE_BUCKET}" && -n "${CLICKHOUSE_PREFIX}" ]]; then
  if [[ -f "$RESTORE_MARKER" ]]; then
    echo "[userdata] ClickHouse manual restore already performed; skipping."
    log_step "ClickHouse restore-from-S3 skipped (marker present)"
  else
    echo "[userdata] Looking for latest manual-* ClickHouse backup in S3..."
    # Expect folders like: manual-20251109T123456Z/
    LATEST_CH_BACKUP="$(aws s3 ls "s3://${CLICKHOUSE_BUCKET}/${CLICKHOUSE_PREFIX}/backups/" \
      | awk '/manual-/{print $2}' \
      | sort \
      | tail -n 1 || true)"

    if [[ -n "$LATEST_CH_BACKUP" ]]; then
      echo "[userdata] Found manual backup: $LATEST_CH_BACKUP"

      # Ensure s3_backups disk is configured in ClickHouse
      HAS_S3_DISK="$(clickhouse-client -q "SELECT count() FROM system.disks WHERE name = 's3_backups'" || echo 0)"
      if [[ "$HAS_S3_DISK" -eq 1 ]]; then
        echo "[userdata] Restoring ClickHouse DATABASE default from Disk('s3_backups', '$LATEST_CH_BACKUP')..."
        clickhouse-client --multiquery -q "
          RESTORE DATABASE default AS default
          FROM Disk('s3_backups', '$LATEST_CH_BACKUP')
          SETTINGS allow_non_empty_tables = 1;
        " && touch "$RESTORE_MARKER" || echo "[userdata] WARNING: ClickHouse RESTORE failed"
        log_step "ClickHouse RESTORE attempted from $LATEST_CH_BACKUP"
      else
        echo "[userdata] s3_backups disk not present in system.disks; skipping restore."
        log_step "ClickHouse RESTORE skipped (no s3_backups disk)"
      fi
    else
      echo "[userdata] No manual-* ClickHouse backups found in S3; starting fresh."
      log_step "ClickHouse RESTORE skipped (no manual-* backup)"
    fi
  fi
else
  echo "[userdata] Skipping ClickHouse restore check (need AWS_REGION/CLICKHOUSE_BUCKET/CLICKHOUSE_PREFIX)"
  log_step "ClickHouse restore check skipped (no AWS config)"
fi

# ---------------- Manual backup script (no timer) -------
install -m 0755 -d /usr/local/bin
cat >/usr/local/bin/clickhouse-backup.sh <<'EOF'
#!/usr/bin/env bash
set -euo pipefail
TS="$(date -u +%Y%m%dT%H%M%SZ)"

# Ensure s3_backups disk exists before attempting backup
HAS_S3_DISK="$(clickhouse-client -q "SELECT count() FROM system.disks WHERE name = 's3_backups'" || echo 0)"
if [[ "$HAS_S3_DISK" -ne 1 ]]; then
  echo "s3_backups disk not configured; cannot run ClickHouse backup."
  exit 1
fi

clickhouse-client --multiquery -q "
  BACKUP DATABASE default
  TO Disk('s3_backups', 'manual-$TS/');
"
echo "Created ClickHouse BACKUP DATABASE default TO Disk('s3_backups', 'manual-$TS/')"
EOF
chmod +x /usr/local/bin/clickhouse-backup.sh
echo "[userdata] Manual ClickHouse backup script installed at /usr/local/bin/clickhouse-backup.sh"
log_step "clickhouse-backup.sh installed"

# Run Prometheus/Grafana bootstrap script
echo "[userdata] downloading Prometheus-Grafana bootstrap script from S3..."
aws s3 cp "s3://${CLICKHOUSE_BUCKET}/${CLICKHOUSE_PREFIX}/scripts/prometheus-grafana-bootstrap.sh" /usr/local/bin/prometheus-grafana-bootstrap.sh
chmod +x /usr/local/bin/prometheus-grafana-bootstrap.sh
/usr/local/bin/prometheus-grafana-bootstrap.sh > /var/log/prometheus-grafana-bootstrap.log 2>&1
log_step "prometheus-grafana-bootstrap.sh executed"

# ---- Quick logs / ports ----
ss -lntp | egrep '(:9090|:3000|:9100|:9363)' || true
echo "[userdata:obs] Prometheus=9090 Grafana=3000 NodeExp=9100 CH-metrics=9363 configured"

# Non-fatal reachability checks (these may fail until SGs are correct)
curl -fsS "http://${MONGO_HOST}:${MONGO_EXP_PORT}/metrics" | head -n 3 || true
curl -fsS "http://${REDPANDA_HOST}:${REDPANDA_EXP_PORT}/metrics" | head -n 3 || true
curl -fsS "http://${MONGO_HOST}:${MONGO_NODE_PORT}/metrics" | head -n 3 || true
curl -fsS "http://${REDPANDA_HOST}:${REDPANDA_NODE_PORT}/metrics" | head -n 3 || true
log_step "observability checks and remote metrics curls"

# ---------------- Marker -------------------------------
echo "$(date -u +%FT%TZ) BOOTSTRAP_OK" > "$MARKER_FILE"
echo "[userdata] done $(date -Is)"
log_step "script complete (BOOTSTRAP_OK written)"
